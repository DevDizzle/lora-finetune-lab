# ðŸ§¬ LoRA Fine-Tune Lab

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![HuggingFace](https://img.shields.io/badge/ðŸ¤—-Transformers-yellow.svg)](https://huggingface.co/docs/transformers)
[![PEFT](https://img.shields.io/badge/PEFT-LoRA-green.svg)](https://huggingface.co/docs/peft)
[![License: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)

**Parameter-efficient fine-tuning of Mistral-7B using LoRA**, demonstrating production-grade ML engineering: dataset preparation, 4-bit quantized training, evaluation with automated metrics, and efficient inference serving.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LoRA Fine-Tune Lab                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Data Prep   â”‚â”€â”€â”€â–¶â”‚  Fine-Tune   â”‚â”€â”€â”€â–¶â”‚    Evaluation    â”‚  â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚                  â”‚  â”‚
â”‚  â”‚ â€¢ Load HF    â”‚    â”‚ â€¢ QLoRA 4bit â”‚    â”‚ â€¢ Perplexity     â”‚  â”‚
â”‚  â”‚   dataset    â”‚    â”‚ â€¢ r=16 Î±=32  â”‚    â”‚ â€¢ ROUGE scores   â”‚  â”‚
â”‚  â”‚ â€¢ Tokenize   â”‚    â”‚ â€¢ Grad accum â”‚    â”‚ â€¢ Task metrics   â”‚  â”‚
â”‚  â”‚ â€¢ Train/val  â”‚    â”‚ â€¢ Checkpointsâ”‚    â”‚ â€¢ W&B logging    â”‚  â”‚
â”‚  â”‚   split      â”‚    â”‚              â”‚    â”‚                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚                                   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚   LoRA Adapter  â”‚                          â”‚
â”‚                    â”‚   (~16 MB)      â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                             â”‚                                   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚   Inference     â”‚                          â”‚
â”‚                    â”‚   CLI / API     â”‚                          â”‚
â”‚                    â”‚                 â”‚                          â”‚
â”‚                    â”‚ Base + Adapter  â”‚                          â”‚
â”‚                    â”‚ 4-bit quantized â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tech: Transformers Â· PEFT Â· Accelerate Â· bitsandbytes Â· W&B   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## What This Demonstrates

| Skill | Implementation |
|-------|---------------|
| **Parameter-efficient fine-tuning** | LoRA with rank-16 decomposition on attention projections |
| **Memory-efficient training** | QLoRA 4-bit quantization via bitsandbytes NF4 |
| **Training engineering** | Gradient accumulation, cosine scheduling, warmup |
| **Evaluation rigor** | Perplexity, ROUGE-L, task-specific accuracy |
| **Experiment tracking** | Weights & Biases integration with artifact logging |
| **Production inference** | Merged adapter loading with CLI interface |
| **Reproducibility** | YAML configs, fixed seeds, pinned dependencies |

## Quick Start

```bash
# Clone and install
git clone https://github.com/YOUR_USER/lora-finetune-lab.git
cd lora-finetune-lab
pip install -r requirements.txt

# Prepare data
python src/data_prep.py --dataset tatsu-lab/alpaca --output data/

# Fine-tune (requires GPU â€” see GUIDE.md)
python -m notebooks.finetune

# Evaluate
python -m notebooks.evaluate

# Run inference
python src/inference.py --adapter outputs/lora-adapter --prompt "Explain LoRA in one paragraph."
```

## Project Structure

```
lora-finetune-lab/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ lora_config.yaml          # LoRA hyperparameters
â”‚   â””â”€â”€ training_config.yaml      # Training loop config
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ finetune.py               # Fine-tuning notebook (percent script)
â”‚   â””â”€â”€ evaluate.py               # Evaluation notebook (percent script)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py              # Dataset preprocessing
â”‚   â””â”€â”€ inference.py              # Inference CLI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ GUIDE.md                      # Step-by-step execution guide
â””â”€â”€ README.md
```

## Results

**Run Date:** February 12, 2026
**Hardware:** NVIDIA A100
**Training Time:** ~3 hours (2h 59m 54s)
**WandB Report:** [View Run Logs](https://wandb.ai/eraphaelparra-evanparra-ai/lora-finetune-lab/runs/0gxeyqwe)

| Metric | Value | Notes |
|--------|-------|-------|
| **Eval Loss** | 1.187 | Low validation loss indicates good generalization |
| **Eval Entropy** | 0.888 | Measure of prediction uncertainty |
| **Token Accuracy** | 70.78% | Percentage of tokens predicted correctly |
| **Train Loss** | 0.890 | Final training loss after 3 epochs |

**Example Inference:**
> **Instruction:** Write a python function to reverse a list.
>
> **Response:**
> ```python
> def reverse_list(lst):
>    return lst[::-1]
> ```

**Training Details:**
- Base model: `mistralai/Mistral-7B-v0.3`
- Trainable parameters: ~4.2M / 7.2B (0.06%)
- Epochs: 3
- Speed: 13.73 samples/sec (train), 61.35 samples/sec (eval)

## License

MIT
